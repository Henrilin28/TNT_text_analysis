{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from TNT_Api import TNT_Api\n",
    "UN = TNT_Api(tag='聯合國',nlimit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377 documents found in tag 聯合國\n",
      "downloading, 10000 to go\n",
      "0 data downloaded\n",
      "100 data downloaded\n",
      "200 data downloaded\n",
      "300 data downloaded\n",
      "400 data downloaded\n",
      "500 data downloaded\n",
      "600 data downloaded\n",
      "700 data downloaded\n",
      "800 data downloaded\n",
      "900 data downloaded\n",
      "1000 data downloaded\n",
      "1100 data downloaded\n",
      "1200 data downloaded\n",
      "1300 data downloaded\n"
     ]
    }
   ],
   "source": [
    "UN.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Simple Cleaning and Spelling Correction\n",
    "ocr = UN.ocr.copy().reset_index()\n",
    "def replace_line_change(t):\n",
    "    if len(t)>0: return t[0].replace('\\n',' ')\n",
    "    else: return None\n",
    "ocr['ocr_text'] = ocr.ocr.map(replace_line_change,na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Remove non-English\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect import detect_langs\n",
    "DetectorFactory.seed =0 \n",
    "def remove_non_eng(text):\n",
    "    if detect(text)=='en':return text\n",
    "    else: return None\n",
    "ocr['ocr_eng'] = ocr.ocr_text.map(remove_non_eng,na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ocr = ocr.dropna(axis=0,how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ocr.to_csv('data/UN_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/UNcorpora_eng.txt.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-650b7bc08fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspelling_correction\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msplng_cr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mchkr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_US'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DataScience/NationalTreasure/spelling_correction.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mWORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/UNcorpora_eng.txt.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#WORDS = Counter(words(open('data/big.txt.txt').read()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/UNcorpora_eng.txt.txt'"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "chkr = enchant.Dict('en_US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## test cell\n",
    "import spelling_correction as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test = ocr.ocr_eng.copy()\n",
    "def check_mis_spell(text):\n",
    "    mis_rate = np.sum([chkr.check(w) for w in sc.words(text)])/len(sc.words(text))\n",
    "    return mis_rate\n",
    "mis_rate = test.map(check_mis_spell,na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mis_rate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_length = test.map(len,na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum([text_length>100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words = text_length.sum()\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def auto_correct(text):\n",
    "    wordlist = list()\n",
    "    for w in sc.words(text):\n",
    "        if chkr.check(w): wordlist.append(w)\n",
    "        else: wordlist.append(sc.correction(w))\n",
    "    art = \" \".join(wordlist)\n",
    "    return art\n",
    "import time\n",
    "start = time.time()\n",
    "subset = test.map(auto_correct,na_action='ignore')\n",
    "end = time.time()-start\n",
    "print (end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_mis_spell(text):\n",
    "    mis_rate = np.sum([chkr.check(w) for w in sc.words(text)])/len(sc.words(text))\n",
    "    return mis_rate\n",
    "mis_rate = test.map(subset,na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      activities for regional nurses the regional nu...\n",
       "1      to hug lien key map legend chingshu existing p...\n",
       "2      the office of ohio t laepeetor serene that dor...\n",
       "4      b ground water development in the muslin and c...\n",
       "5      ten page 10 sodium bromide phonoborbital ferri...\n",
       "6      to pumps the present system of open world bidd...\n",
       "7      agency operating inst action carry tree patien...\n",
       "8      1 may 22 1946 memory nd up to from eager m wah...\n",
       "9      16 august 196 subjects woolly operations table...\n",
       "10     1 appendix d medial schools hospitals and publ...\n",
       "12     30 the western 811trvta2 pains here had floods...\n",
       "13     intra shanghai regional office 6 february 1946...\n",
       "14     d equipment 14 the plan of operation also prov...\n",
       "15     united nations relief and rehabilitation admin...\n",
       "16     4 only fat been presented in final for for n o...\n",
       "17     race deposit along the right abutment appears ...\n",
       "18     labors conveniently and praetteably in the 1ab...\n",
       "19     page 2 dr sa under mentioned thacfx the fact t...\n",
       "21     shanghats regional attire monthly lloport june...\n",
       "23     downstream a short distance three additional b...\n",
       "24     would be kept at peking at all flood flows and...\n",
       "25     agency operating institution carry 501 200 fre...\n",
       "27     chinese national relief and rehabi11tation adm...\n",
       "28     i mr k t ch director department of water conse...\n",
       "29     inventory of supplies received from intra thro...\n",
       "30     a station history should be kept describing ev...\n",
       "31     ogg investigations and studies 14 when the spe...\n",
       "32     sulfaguano dine tula heroine sulfandil made an...\n",
       "33     the pingtung irrigation association has made m...\n",
       "34     ground water eve moment ground water ground wa...\n",
       "                             ...                        \n",
       "77     3 proceed to hengyang to clarify the situation...\n",
       "78     18 the laboratory occupies space in two buildi...\n",
       "79     ely report o 18 training section appendix b co...\n",
       "80     the originally cold dry air is modified to moi...\n",
       "81     and will accomplish more much thought must go ...\n",
       "82     river 6 lower tanshui river possibilities for ...\n",
       "83     ill flood vere effects could be expected to ma...\n",
       "84     an radiotherapy facilities 111 senegal prelimi...\n",
       "85     chinese national relief and rehabilitation adm...\n",
       "86     ba follow up actions from meeting 25 october 1...\n",
       "87     dividend by industrial organtratione during th...\n",
       "88     2 john g johnson jr 2 ay 1 1946 5 the understa...\n",
       "89     united nations relief and rehabilitation admin...\n",
       "90     if that proved satin story that would beeches ...\n",
       "91     1r shneldoton industr1al road horror returned ...\n",
       "92     chinese national seller and rehab1litation adm...\n",
       "93     10 pharmnoeutical service available to reports...\n",
       "94     4 shanghai medical college term commenced for ...\n",
       "95     6 needle look type tonsil straight 6 need spin...\n",
       "96     and in no wife considered an adjunct to the ne...\n",
       "97     37 most typhoons that hit taiwan originate in ...\n",
       "98     chinese national relief and rehabilitation adm...\n",
       "99     of work 8k the s fronted with major problems t...\n",
       "100    remarked 1 these village are within a plain wh...\n",
       "101    major generaユ01en e egerton page to land chief...\n",
       "102    not to be r chinese national din relief and re...\n",
       "103    23 this agreement is made in sextuplet each pa...\n",
       "104    20 hay 1946 beg ona2 0oordination蹉vision repor...\n",
       "105    medial training in obstetrice and cneeolo r lo...\n",
       "106    red nations relief and rghabiltaion tin minute...\n",
       "Name: ocr_eng, Length: 100, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spelling_correction as sc\n",
    "def correct(text):\n",
    "    return \" \".join(map(sc.correction,sc.words(text)))\n",
    "test['ocr_corrected'] = test.ocr_text.map(correct,na_action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "documents = list(ocr)\n",
    "no_features = 200\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "no_topics = 3\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "print \"==============================\"\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
