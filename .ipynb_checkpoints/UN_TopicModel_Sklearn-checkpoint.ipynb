{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from TNT_Api import TNT_Api\n",
    "UN = TNT_Api(tag='聯合國',nlimit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377 documents found in tag 聯合國\n",
      "downloading, 100000 to go\n",
      "0 data downloaded\n",
      "100 data downloaded\n",
      "200 data downloaded\n",
      "300 data downloaded\n",
      "400 data downloaded\n",
      "500 data downloaded\n",
      "600 data downloaded\n",
      "700 data downloaded\n",
      "800 data downloaded\n",
      "900 data downloaded\n",
      "1000 data downloaded\n",
      "1100 data downloaded\n",
      "1200 data downloaded\n",
      "1300 data downloaded\n"
     ]
    }
   ],
   "source": [
    "UN.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Simple Cleaning and Spelling Correction\n",
    "ocr = UN.ocr.copy().reset_index()\n",
    "def replace_line_change(t):\n",
    "    if len(t)>0: return t[0].replace('\\n',' ')\n",
    "    else: return None\n",
    "ocr['ocr_text'] = ocr.ocr.map(replace_line_change,na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Remove non-English\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect import detect_langs\n",
    "DetectorFactory.seed =0 \n",
    "def remove_non_eng(text):\n",
    "    if detect(text)=='en':return text\n",
    "    else: return None\n",
    "ocr['ocr_eng'] = ocr.ocr_text.map(remove_non_eng,na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ocr.ocr_eng.isnull()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ocr = ocr.dropna(axis=0,how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "import spelling_correction as splng_cr\n",
    "chkr = enchant.Dict('en_US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## test cell\n",
    "import spelling_correction as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test = ocr.ocr_eng.copy()\n",
    "def check_mis_spell(text):\n",
    "    mis_rate = np.sum([chkr.check(w) for w in sc.words(text)])/len(sc.words(text))\n",
    "    return mis_rate\n",
    "mis_rate = test.map(check_mis_spell,na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1284.000000\n",
       "mean        0.744819\n",
       "std         0.129981\n",
       "min         0.142857\n",
       "25%         0.647444\n",
       "50%         0.763569\n",
       "75%         0.849251\n",
       "max         1.000000\n",
       "Name: ocr_eng, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis_rate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_length = test.map(len,na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1262"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([text_length>100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2199153"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = text_length.sum()\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225.57488632202148\n"
     ]
    }
   ],
   "source": [
    "def auto_correct(text):\n",
    "    wordlist = list()\n",
    "    for w in sc.words(text):\n",
    "        if chkr.check(w): wordlist.append(w)\n",
    "        else: wordlist.append(sc.correction(w))\n",
    "    art = \" \".join(wordlist)\n",
    "    return art\n",
    "import time\n",
    "start = time.time()\n",
    "subset = test.iloc[:100].map(auto_correct,na_action='ignore')\n",
    "end = time.time()-start\n",
    "print (end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    activities for regional nurses the regional nu...\n",
       "1    to hug lien key map legend chingshu existing p...\n",
       "2    the office of ohio t laepeetor serene that dor...\n",
       "4    b ground water development in the muslin and c...\n",
       "5    ten page 10 sodium bromide phonoborbital ferri...\n",
       "Name: ocr_eng, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import digits\n",
    "remove_digits = str.maketrans('','',digits)\n",
    "\n",
    "def remove_number(text):\n",
    "    return text.translate(remove_digits)\n",
    "subset = subset.map(remove_number,na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset.to_csv('data/UN_subset_checkpoint.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ocr_eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>activities for regional nurses the regional nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to hug lien key map legend chingshu existing p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the office of ohio t laepeetor serene that dor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b ground water development in the muslin and c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ten page 10 sodium bromide phonoborbital ferri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ocr_eng\n",
       "0  activities for regional nurses the regional nu...\n",
       "1  to hug lien key map legend chingshu existing p...\n",
       "2  the office of ohio t laepeetor serene that dor...\n",
       "3  b ground water development in the muslin and c...\n",
       "4  ten page 10 sodium bromide phonoborbital ferri..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.plotting import save\n",
    "from bokeh.models import HoverTool\n",
    "UN_full_text = pd.read_csv('data/UN_full_text.csv')\n",
    "UN_full_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from string import digits\n",
    "remove_digits = str.maketrans('','',digits)\n",
    "\n",
    "def remove_number(text):\n",
    "    return text.translate(remove_digits)\n",
    "UN_full_text = UN_full_text.ocr_eng.map(remove_number,na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = list(UN_full_text.ocr_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yhhsu/Anaconda/anaconda/envs/NLP/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Non-Negative Matrix Factorization===========\n",
      "Topic 0:\n",
      "carry intra supplies office regional personnel china program relief director\n",
      "Topic 1:\n",
      "river sediment flood stations basin data site areas later bed\n",
      "Topic 2:\n",
      "hospital nurses nursing medical beds health red 10 shanghai cases\n",
      "Topic 3:\n",
      "dr medical health supplies division meeting report action shanghai present\n",
      "Topic 4:\n",
      "000 50 tons 10 20 total 100 12 people years\n",
      "Topic 5:\n",
      "shop branch national central new meeting 50 total plan equipment\n",
      "Topic 6:\n",
      "grade officer regional 12 division chief 11 10 100 field\n",
      "Topic 7:\n",
      "tho th end tons government people distribution time island control\n",
      "Topic 8:\n",
      "mr meeting director china committee office intra chief staff plan\n",
      "Topic 9:\n",
      "hydraulic laboratory equipment work development problems planning taiwan available government\n",
      "========Latent Dirichlet Allocation=================\n",
      "Topic 0:\n",
      "stations 100 used taiwan tons island period years general new\n",
      "Topic 1:\n",
      "river sediment flood bed control data areas present later basin\n",
      "Topic 2:\n",
      "dr medical supplies health shanghai program china report intra present\n",
      "Topic 3:\n",
      "000 10 50 20 total 12 100 area 11 cases\n",
      "Topic 4:\n",
      "equipment laboratory hydraulic work development staff problems field available need\n",
      "Topic 5:\n",
      "hospital health medical nurses nursing red personnel training program supplies\n",
      "Topic 6:\n",
      "mr shop branch meeting new national central plan china intra\n",
      "Topic 7:\n",
      "carry intra office supplies director china work relief general personnel\n",
      "Topic 8:\n",
      "regional officer grade personnel division chief office 12 11 report\n",
      "Topic 9:\n",
      "tho people th end time ton taiwan control carry conditions\n"
     ]
    }
   ],
   "source": [
    "no_features = 100\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 10\n",
    "# Run NMF\n",
    "NMF = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "# Run LDA\n",
    "LDA = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\\\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "print (\"========Non-Negative Matrix Factorization===========\")\n",
    "display_topics(NMF, tfidf_feature_names, no_top_words)\n",
    "print (\"========Latent Dirichlet Allocation=================\")\n",
    "display_topics(LDA, tf_feature_names, no_top_words)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
