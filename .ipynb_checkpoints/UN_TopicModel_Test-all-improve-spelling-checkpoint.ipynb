{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from TNT_Api import TNT_Api\n",
    "UN = TNT_Api(tag='聯合國',nlimit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377 documents found in tag 聯合國\n",
      "downloading, 10000 to go\n",
      "0 data downloaded\n",
      "100 data downloaded\n",
      "200 data downloaded\n",
      "300 data downloaded\n",
      "400 data downloaded\n",
      "500 data downloaded\n",
      "600 data downloaded\n",
      "700 data downloaded\n",
      "800 data downloaded\n",
      "900 data downloaded\n",
      "1000 data downloaded\n",
      "1100 data downloaded\n",
      "1200 data downloaded\n",
      "1300 data downloaded\n"
     ]
    }
   ],
   "source": [
    "UN.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Simple Cleaning and Spelling Correction\n",
    "ocr = UN.ocr.copy().reset_index()\n",
    "def replace_line_change(t):\n",
    "    if len(t)>0: return t[0].replace('\\n',' ')\n",
    "    else: return None\n",
    "ocr['ocr_text'] = ocr.ocr.map(replace_line_change,na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Remove non-English\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect import detect_langs\n",
    "DetectorFactory.seed =0 \n",
    "def remove_non_eng(text):\n",
    "    if detect(text)=='en':return text\n",
    "    else: return None\n",
    "ocr['ocr_eng'] = ocr.ocr_text.map(remove_non_eng,na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ocr = ocr.dropna(axis=0,how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ocr.to_csv('data/UN_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "import spelling_correction as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "ocr = pd.read_csv('data/UN_raw.csv')\n",
    "chkr = enchant.Dict('en_US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test = ocr.ocr_eng.copy()\n",
    "def check_mis_spell(text):\n",
    "    mis_rate = np.sum([chkr.check(w) for w in sc.words(text)])/len(sc.words(text))\n",
    "    return mis_rate\n",
    "mis_rate = test.map(check_mis_spell,na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1284.000000\n",
       "mean        0.744819\n",
       "std         0.129981\n",
       "min         0.142857\n",
       "25%         0.647444\n",
       "50%         0.763569\n",
       "75%         0.849251\n",
       "max         1.000000\n",
       "Name: ocr_eng, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis_rate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 1284 documents and 2199153 words\n"
     ]
    }
   ],
   "source": [
    "text_length = test.map(len,na_action='ignore')\n",
    "total_words = text_length.sum()\n",
    "print ('There are total {:d} documents and {:d} words'.format(len(text_length),text_length.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def auto_correct(text):\n",
    "    wordlist = []\n",
    "    correct_dic = {}\n",
    "    for w in sc.words(text):\n",
    "        if chkr.check(w): wordlist.append(w)\n",
    "        else: \n",
    "            c = sc.correction(w)\n",
    "            correct_dic[w]=c\n",
    "    art = \" \".join(wordlist)\n",
    "    return art,correct_dic\n",
    "start = time.time()\n",
    "subset = test.map(auto_correct,na_action='ignore')\n",
    "end = time.time()-start\n",
    "print (end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "documents = list(ocr)\n",
    "no_features = 200\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "no_topics = 3\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "print \"==============================\"\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
